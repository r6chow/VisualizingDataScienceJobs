{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference https://towardsdatascience.com/scraping-job-posting-data-from-indeed-using-selenium-and-beautifulsoup-dfc86230baac\n",
    "\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "FETCH_DELAY_SECONDS = 1 # or whatever value you're comfortable with\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"\n",
    "    Given the url of a page, this function returns the soup object.\n",
    "    \n",
    "    Parameters:\n",
    "        url: the link to get soup object for\n",
    "    \n",
    "    Returns:\n",
    "        soup: soup object\n",
    "    \"\"\"\n",
    "#     driver = webdriver.Firefox()\n",
    "#     driver.get(url)\n",
    "#     html = driver.page_source\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "#     driver.close()\n",
    "\n",
    "    sleep(FETCH_DELAY_SECONDS)\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser') \n",
    "    #soup = BeautifulSoup(req.text, \"lxml\", from_encoding=\"utf-8\")\n",
    "    return soup\n",
    "\n",
    "\n",
    "def grab_job_locs(soup):\n",
    "    locs = []\n",
    "    \n",
    "    # Loop thru all the posting links\n",
    "    for span in soup.find_all('span', {'class': 'location'}):\n",
    "        # Since sponsored job postings are represented by \"a target\" instead of \"a href\", no need to worry here\n",
    "        loc = span.text\n",
    "        locs.append(loc)\n",
    "    \n",
    "    return locs\n",
    "\n",
    "\n",
    "\n",
    "def grab_job_links_v1(soup, baseurl):\n",
    "    \"\"\"\n",
    "    Grab all non-sponsored job posting links from a Indeed search result page using the given soup object\n",
    "    \n",
    "    Parameters:\n",
    "        soup: the soup object corresponding to a search result page\n",
    "                e.g. https://ca.indeed.com/jobs?q=data+scientist&l=Toronto&start=20\n",
    "    \n",
    "    Returns:\n",
    "        urls: a python list of job posting urls\n",
    "    \n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    \n",
    "    # Loop thru all the posting links\n",
    "    for link in soup.find_all('h2', {'class': 'jobtitle'}):\n",
    "          \n",
    "        # Since sponsored job postings are represented by \"a target\" instead of \"a href\", no need to worry here\n",
    "        partial_url = link.a.get('href')\n",
    "        # This is a partial url, we need to attach the prefix\n",
    "        url = baseurl + partial_url\n",
    "        # Make sure this is not a sponsored posting\n",
    "        \n",
    "        urls.append(url)\n",
    "    \n",
    "    for link in soup.find_all('div', {'class': 'title'}):\n",
    "      \n",
    "        # Since sponsored job postings are represented by \"a target\" instead of \"a href\", no need to worry here\n",
    "        partial_url = link.a.get('href')\n",
    "        # This is a partial url, we need to attach the prefix\n",
    "        url = baseurl + partial_url\n",
    "        # Make sure this is not a sponsored posting\n",
    "        \n",
    "        urls.append(url)\n",
    "    \n",
    "    \n",
    "    #print(\"grab_job_links num_urls:\", len(urls))\n",
    "    return urls\n",
    "\n",
    "\n",
    "def grab_job_links(soup, baseurl):\n",
    "    \"\"\"\n",
    "    Grab all non-sponsored job posting links from a Indeed search result page using the given soup object\n",
    "    \n",
    "    Parameters:\n",
    "        soup: the soup object corresponding to a search result page\n",
    "                e.g. https://ca.indeed.com/jobs?q=data+scientist&l=Toronto&start=20\n",
    "    \n",
    "    Returns:\n",
    "        urls: a python list of job posting urls\n",
    "    \n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    \n",
    "    for div_row in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}): \n",
    "        \n",
    "        #check sponsored   \n",
    "        div_sponsor = div_row.find('span', {'class': 'sponsoredGray'})\n",
    "        div_resultlink = div_row.find('span', {'class': 'result-link'})\n",
    "        if ((div_sponsor is not None) and (div_resultlink is None)):\n",
    "            sponsored_val = 'Sponsored'    \n",
    "        else:\n",
    "            sponsored_val = ''\n",
    "        \n",
    "        \n",
    "        #salary\n",
    "        div_salary = div_row.find('div', {'class':'salarySnippet'})\n",
    "        if (div_salary is not None):\n",
    "            salary = div_salary.get_text(' ')    \n",
    "        else:\n",
    "            salary = ''\n",
    "        \n",
    "        \n",
    "        #summary\n",
    "        div_summary = div_row.find('div', {'class':'summary'})\n",
    "        span_summary = div_row.find('span', {'class':'summary'})\n",
    "        if (div_summary is not None):\n",
    "            summary = div_summary.get_text(' ')    \n",
    "        elif (span_summary is not None):\n",
    "            summary = span_summary.get_text(' ') \n",
    "        else:\n",
    "            summary = ''\n",
    "        \n",
    "        \n",
    "        url = ''\n",
    "        \n",
    "        h_title = div_row.find('h2', {'class':'jobtitle'})\n",
    "        if (h_title is not None):\n",
    "            partial_url = h_title.a.get('href')\n",
    "            url = baseurl + partial_url\n",
    "        \n",
    "        div_title = div_row.find('div', {'class': 'title'})\n",
    "        if (div_title is not None):\n",
    "            partial_url = div_title.a.get('href')\n",
    "            url = baseurl + partial_url\n",
    "            \n",
    "        if (url != ''):\n",
    "            urls.append({'url':url,'sponsored_flg':sponsored_val, 'salary': salary, 'summary':summary})\n",
    "            \n",
    "    \n",
    "#     # Loop thru all the posting links\n",
    "#     for link in soup.find_all('h2', {'class': 'jobtitle'}):\n",
    "          \n",
    "#         # Since sponsored job postings are represented by \"a target\" instead of \"a href\", no need to worry here\n",
    "#         partial_url = link.a.get('href')\n",
    "#         # This is a partial url, we need to attach the prefix\n",
    "#         url = baseurl + partial_url\n",
    "#         # Make sure this is not a sponsored posting\n",
    "        \n",
    "#         urls.append(url)\n",
    "    \n",
    "#     for link in soup.find_all('div', {'class': 'title'}):\n",
    "      \n",
    "#         # Since sponsored job postings are represented by \"a target\" instead of \"a href\", no need to worry here\n",
    "#         partial_url = link.a.get('href')\n",
    "#         # This is a partial url, we need to attach the prefix\n",
    "#         url = baseurl + partial_url\n",
    "#         # Make sure this is not a sponsored posting\n",
    "        \n",
    "#         urls.append(url)\n",
    "    \n",
    "    \n",
    "    #print(\"grab_job_links num_urls:\", len(urls))\n",
    "    return urls\n",
    "\n",
    "\n",
    "\n",
    "def get_urls(query, num_pages, location, base_site):\n",
    "    \"\"\"\n",
    "    Get all the job posting URLs resulted from a specific search.\n",
    "    \n",
    "    Parameters:\n",
    "        query: job title to query\n",
    "        num_pages: number of pages needed\n",
    "        location: city to search in\n",
    "    \n",
    "    Returns:\n",
    "        urls: a list of job posting URL's (when num_pages valid)\n",
    "        max_pages: maximum number of pages allowed ((when num_pages invalid))\n",
    "    \"\"\"\n",
    "    # We always need the first page\n",
    "    base_url = '{}/jobs?q={}&l={}'.format(base_site, query, location)\n",
    "    soup = get_soup(base_url)\n",
    "    \n",
    "    urls = []\n",
    "    locs = []\n",
    "    \n",
    "    print(\"base url:\",base_url)\n",
    "    # Get the total number of postings found \n",
    "    posting_count_string = soup.find(name='div', attrs={'id':\"searchCount\"}).get_text()\n",
    "    posting_count_string = posting_count_string[posting_count_string.find('of')+2:].strip().replace(\",\",\"\")\n",
    "    \n",
    "    try:\n",
    "        posting_count = int(posting_count_string)\n",
    "    except ValueError: # deal with special case when parsed string is \"360 jobs\"\n",
    "        posting_count = int(re.search('\\d+', posting_count_string).group(0))\n",
    "    \n",
    "    # Limit nunmber of pages to get\n",
    "    max_pages = round(posting_count / 10)\n",
    "    if num_pages > max_pages:\n",
    "        #print('returning max_pages!!')\n",
    "        #return max_pages\n",
    "        num_pages = max_pages\n",
    "    \n",
    "    print(\"posting count:{}  num pages:{}\".format(posting_count, num_pages))\n",
    "    # Additional work is needed when more than 1 page is requested\n",
    "\n",
    "    for i in range(num_pages):\n",
    "        num = (i) * 10\n",
    "        base_url = '{}/jobs?q={}&l={}&start={}'.format(base_site,query, location, num)\n",
    "        #print(\"base:\", base_url)\n",
    "        soup = get_soup(base_url)\n",
    "        # We always combine the results back to the list\n",
    "        urls += grab_job_links(soup, base_site)\n",
    "        locs += grab_job_locs(soup)\n",
    "        print(\"Grab Progress: {}/{}  {:2.0f}%\".format(i+1, num_pages, 100*((i+1)/num_pages)), end='\\r')\n",
    "\n",
    "    # Check to ensure the number of urls gotten is correct\n",
    "    #assert len(urls) == num_pages * 10, \"There are missing job links, check code!\"\n",
    "\n",
    "    #print(\"urls:\", len(urls))\n",
    "    return {'urls': urls, 'locs': locs}     \n",
    "\n",
    "\n",
    "\n",
    "def get_posting(url):\n",
    "    \"\"\"\n",
    "    Get the text portion including both title and job description of the job posting from a given url\n",
    "    \n",
    "    Parameters:\n",
    "        url: The job posting link\n",
    "        \n",
    "    Returns:\n",
    "        title: the job title (if \"data scientist\" is in the title)\n",
    "        posting: the job posting content    \n",
    "    \"\"\"\n",
    "    # Get the url content as BS object\n",
    "    soup = get_soup(url)\n",
    "    \n",
    "    # The job title is held in the h3 tag\n",
    "    title = soup.find(name='h3').get_text()\n",
    "    posting = soup.find(name='div', attrs={'class': \"jobsearch-JobComponent-description\"}).get_text(' ')\n",
    "    \n",
    "    div = soup.find('div', {'class': 'jobsearch-InlineCompanyRating'})\n",
    "    company_divs = div.find_all('div')\n",
    "    \n",
    "    company = ''\n",
    "    if (len(company_divs) > 0):\n",
    "        company = company_divs[0].get_text(' ')\n",
    "    \n",
    "    loc = ''\n",
    "    if (len(company_divs) > 1):\n",
    "        loc = company_divs[-1].get_text(' ')\n",
    "    \n",
    "    return title, posting, company, loc\n",
    "\n",
    "        \n",
    "    #if 'data scientist' in title:  # We'll proceed to grab the job posting text if the title is correct\n",
    "        # All the text info is contained in the div element with the below class, extract the text.\n",
    "        #posting = soup.find(name='div', attrs={'class': \"jobsearch-JobComponent\"}).get_text()\n",
    "        #return title, posting.lower()\n",
    "    #else:\n",
    "        #return False\n",
    "    \n",
    "        # Get rid of numbers and symbols other than given\n",
    "        #text = re.sub(\"[^a-zA-Z'+#&]\", \" \", text)\n",
    "        # Convert to lower case and split to list and then set\n",
    "        #text = text.lower().strip()\n",
    "    \n",
    "        #return text\n",
    "\n",
    "\n",
    "\n",
    "def get_data(query, num_pages, location='', base_site='https://ca.indeed.com', file_prefix = 'CA'):\n",
    "    \n",
    "    print(\"start:\", datetime.datetime.now())\n",
    "    \"\"\"\n",
    "    Get all the job posting data and save in a json file using below structure:\n",
    "    \n",
    "    {<count>: {'title': ..., 'posting':..., 'url':...}...}\n",
    "    \n",
    "    The json file name has this format: \"\"<query>.json\"\n",
    "    \n",
    "    Parameters:\n",
    "        query: Indeed query keyword such as 'Data Scientist'\n",
    "        num_pages: Number of search results needed\n",
    "        location: location to search for\n",
    "    \n",
    "    Returns:\n",
    "        postings_dict: Python dict including all posting data\n",
    "    \n",
    "    \"\"\"\n",
    "    # Convert the queried title to Indeed format\n",
    "    query = '+'.join(query.lower().split())\n",
    "    \n",
    "    postings_dict = {}\n",
    "    \n",
    "    results = get_urls(query, num_pages, location, base_site)\n",
    "    # print(results)\n",
    "    urls = results['urls']\n",
    "    #locs =  results['locs'] \n",
    "    \n",
    "    print()\n",
    "    \n",
    "    #  Continue only if the requested number of pages is valid (when invalid, a number is returned instead of list)\n",
    "    if isinstance(urls, list):\n",
    "        num_urls = len(urls)\n",
    "        for i, url in enumerate(urls):\n",
    "            try:\n",
    "                title, posting, company, loc = get_posting(url['url'])\n",
    "                postings_dict[i] = {}\n",
    "                postings_dict[i]['title'] = title\n",
    "                postings_dict[i]['loc'] = loc\n",
    "                postings_dict[i]['posting'] =  posting\n",
    "                postings_dict[i]['company'] = company \n",
    "                postings_dict[i]['summary'] = url['summary']\n",
    "                postings_dict[i]['salary'] = url['salary']\n",
    "                postings_dict[i]['sponsored_flg'] = url['sponsored_flg']\n",
    "                postings_dict[i]['url'] = url['url']\n",
    "            \n",
    "            except: \n",
    "                continue\n",
    "            \n",
    "            percent = (i+1) / num_urls\n",
    "            # Print the progress the \"end\" arg keeps the message in the same line \n",
    "            print(\"Progress: {}/{}  {:2.0f}%\".format(i+1, num_urls, 100*percent), end='\\r')\n",
    "\n",
    "        # Save the dict as json file\n",
    "        file_name = file_prefix + '_' + query.replace('+', '_').replace(\"(\", \"\").replace(\")\", \"\").replace('\"','') + \"_\" + location + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d%H%M\") + '.json'\n",
    "        with open(file_name, 'w') as f:\n",
    "            json.dump(postings_dict, f)\n",
    "        \n",
    "        print('All {} postings have been scraped and saved!'.format(num_urls))    \n",
    "        #return postings_dict\n",
    "    else:\n",
    "        print(\"Due to similar results, maximum number of pages is only {}. Please try again!\".format(urls))\n",
    "\n",
    "    print(\"finished:\", datetime.datetime.now())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 2019-04-10 23:11:28.509776\n",
      "base url: https://ca.indeed.com/jobs?q=(\"data+scientist\")+or+(\"machine+learning\")&l=\n",
      "posting count:1785  num pages:178\n",
      "Grab Progress: 178/178  100%\n",
      "All 2903 postings have been scraped and saved!\n",
      "finished: 2019-04-11 00:36:57.435256\n",
      "start: 2019-04-11 00:36:57.441239\n",
      "base url: https://au.indeed.com/jobs?q=(\"data+scientist\")+or+(\"machine+learning\")&l=\n",
      "posting count:737  num pages:74\n",
      "Grab Progress: 74/74  100%\n",
      "All 736 postings have been scraped and saved!\n",
      "finished: 2019-04-11 01:02:37.009836\n",
      "start: 2019-04-11 01:02:37.011831\n",
      "base url: https://www.indeed.com.sg/jobs?q=(\"data+scientist\")+or+(\"machine+learning\")&l=\n",
      "posting count:1057  num pages:106\n",
      "Grab Progress: 106/106  100%\n",
      "Progress: 319/1051  30%\r"
     ]
    }
   ],
   "source": [
    "#extract data\n",
    "query = '(\"data scientist\") or (\"machine learning\")'\n",
    "num_pages = 2000\n",
    "get_data(query, num_pages, location='', base_site='https://ca.indeed.com', file_prefix='CA')\n",
    "get_data(query, num_pages, location='', base_site='https://au.indeed.com', file_prefix='AU')\n",
    "get_data(query, num_pages, location='', base_site='https://www.indeed.com.sg', file_prefix='SG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " soup = BeautifulSoup('<br>aaa<br>bbb', 'html.parser') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.get_text(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = ''.join(soup.findAll(text=True))\n",
    "all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ca.indeed.com/viewjob?jk=eae45c8c691026b7&from=serp&vjs=3'\n",
    "soup = get_soup(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The job title is held in the h3 tag\n",
    "title = soup.find(name='h3').get_text()\n",
    "posting = soup.find(name='div', attrs={'class': \"jobsearch-JobComponent-description\"}).get_text(' ')\n",
    "    \n",
    "divs = soup.find('div', {'class': 'jobsearch-InlineCompanyRating'})\n",
    "divs.find_all('div')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_child = div.find(\"div\")\n",
    "div_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
